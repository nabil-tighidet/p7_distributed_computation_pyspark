{"cells": [{"metadata": {"trusted": true}, "id": "e5f0fbe1", "cell_type": "code", "source": "# L'ex\u00e9cution de cette cellule d\u00e9marre l'application Spark", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "id": "27ac9832", "cell_type": "markdown", "source": "# Import des librairies et fonctions"}, {"metadata": {"trusted": true}, "id": "ad562eab", "cell_type": "code", "source": "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras import Model\nfrom pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\nfrom pyspark.sql.types import StructType, StructField, FloatType\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.linalg.distributed import RowMatrix\nfrom pyspark.ml.functions import vector_to_array\nimport pyarrow.parquet as pq\nimport tensorflow as tf\nfrom PIL import Image\nimport pandas as pd, numpy as np\nimport io, os, time\nimport boto3", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Read single parquet file from S3\ndef pd_read_s3_parquet(key, bucket, **args):\n    s3_client = boto3.client('s3')\n    obj = s3_client.get_object(Bucket=bucket, Key=key)\n    return pd.read_parquet(io.BytesIO(obj['Body'].read()), **args)\n\n# Read multiple parquets from a folder on S3 generated by spark\ndef pd_read_s3_multiple_parquets(filepath, bucket, s3=None, verbose=False, **args):\n    s3_client = boto3.client('s3')\n    s3 = boto3.resource('s3')\n    s3_keys = [item.key for item in s3.Bucket(bucket).objects.filter(Prefix=filepath) if item.key.endswith('.parquet')]\n    if not s3_keys:\n        print('No parquet found in', bucket, filepath)\n    elif verbose:\n        print('Load parquets:')\n        for p in s3_keys: \n            print(p)\n    dfs = [pd_read_s3_parquet(key, bucket=bucket, **args) for key in s3_keys]\n    return pd.concat(dfs, ignore_index=True)", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "fb788991", "cell_type": "code", "source": "%%info", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1676211120774_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-43-148.eu-west-1.compute.internal:20888/proxy/application_1676211120774_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-185.eu-west-1.compute.internal:8042/node/containerlogs/container_1676211120774_0001_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Introduction\n\nCe notebook impl\u00e9mente le pr\u00e9processing des images du dataset fruit-360. On souhaite exposer les avantages d'utiliser le calcul distribu\u00e9 pour paral\u00e9liser ses diff\u00e9rentes \u00e9tapes. Pour cela nous utiliserons le framework Spark de calcul distribu\u00e9 avec lequel nous communiquerons via l'API pyspark. \nLe pr\u00e9processing comprend trois \u00e9tapes :\n- Le redimmensionnement des images au format 240 x 240 px\n- L'extraction de features \u00e0 l'aide du mod\u00e8le MobileNetV2\n- La r\u00e9duction de dimension \u00e0 l'aide d'une ACP"}, {"metadata": {}, "id": "83663cbd", "cell_type": "markdown", "source": "# D\u00e9finition des chemins pour les images sources et les r\u00e9sultats\n\nNous acc\u00e9dons directement \u00e0 nos **images sur S3** comme si elles \u00e9taient **stock\u00e9es localement**. On a charg\u00e9 le dataset complet dans le bucket S3 que l'on a rang\u00e9 dans le dossier full_dataset.\nVoici l'arborescence du serveur S3 :\n\nproject-8-data <br>\n\u2502   bootstrap-emr-sh   \n\u2502<br>\n\u2514\u2500\u2500\u2500full_dataset (dossier contenant les images)<br>\n\u2502   \u2502<br>\n\u2502   \u2514\u2500\u2500\u2500Apple_Braeburn<br>\n\u2502   \u2502   \u2502   0_100.jpg<br>\n\u2502   \u2502   \u2502   1_100.jpg<br>\n\u2502   \u2502   \u2502   ...<br>\n\u2502   \u2502<br>\n\u2502   \u2514\u2500\u2500\u2500Apple_Crimson_Snow<br>\n\u2502   \u2502   \u2502   0_100.jpg<br>\n\u2502   \u2502   \u2502   1_100.jpg<br>\n\u2502   \u2502   \u2502   ...<br>\n\u2502   \u2502<br>\n\u2502   ...<br>\n\u2502   \u2502<br>\n\u2502   \u2514\u2500\u2500\u2500Watermelon<br>\n\u2502       \u2502   0_100.jpg<br>\n\u2502       \u2502   1_100.jpg<br>\n\u2502       \u2502   ...<br>\n\u2502<br>\n\u2514\u2500\u2500\u2500results (dossier contenant le r\u00e9sultats du pr\u00e9processing)<br>\n\u2502<br>\n\u2514\u2500\u2500\u2500jupyter (dossier contenant les notebooks)<br>\n\nPour cette d\u00e9monstration, on ne chargera qu'une partie du dataset complet (de 3 \u00e0 35 classes d'imagesce qui correspond \u00e0 600 \u00e0 23000 images) pour rester dans des temps de calculs raisonnables."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "PATH = 's3://project-8-data'\nfolders = boto3.client('s3').list_objects(Bucket='project-8-data', Delimiter='/', Prefix='full_dataset/')\nMULTIPLE_PATH_DATA = [PATH + \"/\" + common_prefix.get('Prefix') for common_prefix in folders.get('CommonPrefixes')]\nPATH_Result = PATH+'/Results'", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "On commence par cr\u00e9er une session Spark en instanciant l'objet python SparkSession. Cet objet va permettre de communiquer avec le cluster manager pour la r\u00e9partition des donn\u00e9es sur les diff\u00e9rents noeuds du cluster et l'allocation des ressources (cpu, ram)."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Cr\u00e9ation de la session Spark\nspark = (SparkSession \\\n             .builder \\\n             .appName('P8') \\\n             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true') \\\n             .getOrCreate())", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "On importe ensuites les images. l'entier n correspond aux nombres de classe de fruit que l'on veut importer. (il ya environ 600 images par classe de fruits) "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Importation des images (0, 3, 15)\nn = 3\nimages = spark.read \\\n            .format(\"binaryFile\") \\\n            .option(\"pathGlobFilter\", \"*.jpg\") \\\n            .option(\"recursiveFileLookup\", \"true\") \\\n            .load(MULTIPLE_PATH_DATA[:n]) \\", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Explication de la distribution \n\nL\u2019extraction de feature \u00e0 l\u2019aide d\u2019un mod\u00e8le tensorflow n\u00e9cessite l\u2019utilisation des poids de ce mod\u00e8le. Donc si on souhaite parall\u00e9liser cette t\u00e2che sur plusieurs n\u0153uds, chaque n\u0153uds doit avoir acc\u00e8s \u00e0 ces poids.\n\u2192 C\u2019est l\u00e0 que le broadcasting entre en jeu. "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Broadcasting des poids du mod\u00e8le sur les diff\u00e9rents noeuds du cluster\n\nmodel = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n\nnew_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nbrodcast_weights = sc.broadcast(new_model.get_weights())", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n\r    8192/14536120 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   49152/14536120 [..............................] - ETA: 36s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   81920/14536120 [..............................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  131072/14536120 [..............................] - ETA: 33s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  180224/14536120 [..............................] - ETA: 30s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  245760/14536120 [..............................] - ETA: 25s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  344064/14536120 [..............................] - ETA: 20s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  458752/14536120 [..............................] - ETA: 16s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  622592/14536120 [>.............................] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  851968/14536120 [>.............................] - ETA: 10s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1163264/14536120 [=>............................] - ETA: 8s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1622016/14536120 [==>...........................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2285568/14536120 [===>..........................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3121152/14536120 [=====>........................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4390912/14536120 [========>.....................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6209536/14536120 [===========>..................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8478720/14536120 [================>.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10321920/14536120 [====================>.........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12484608/14536120 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14467072/14536120 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14540800/14536120 [==============================] - 1s 0us/step", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def model_fn():\n    \"\"\"\n    Returns a MobileNetV2 model with top layer removed \n    and broadcasted pretrained weights.\n    \"\"\"\n    model = MobileNetV2(weights='imagenet',\n                        include_top=True,\n                        input_shape=(224, 224, 3))\n    for layer in model.layers:\n        layer.trainable = False\n    new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n    new_model.set_weights(brodcast_weights.value)\n    return new_model", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Gr\u00e2ce au broadcasting des poids du mod\u00e8le on peut maintenant d\u00e9finir une \u00ab\u00a0user-defined-fonction\u00a0\u00bb gr\u00e2ce au d\u00e9corateur @pandas_udf\nCette fonction va permettre d\u2019appliquer l\u2019extraction de features \u00e0 toutes les images du dataframe de fa\u00e7on distribu\u00e9e."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def preprocess(content):\n    \"\"\"\n    Preprocesses raw image bytes for prediction.\n    \"\"\"\n    img = Image.open(io.BytesIO(content)).resize([224, 224])\n    arr = img_to_array(img)\n    return preprocess_input(arr)\n\ndef featurize_series(model, content_series):\n    \"\"\"\n    Featurize a pd.Series of raw images using the input model.\n    :return: a pd.Series of image features\n    \"\"\"\n    input = np.stack(content_series.map(preprocess))\n    preds = model.predict(input)\n    # For some layers, output features will be multi-dimensional tensors.\n    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n    output = [p.flatten() for p in preds]\n    return pd.Series(output)\n\n@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\ndef featurize_udf(content_series_iter):\n    '''\n    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n\n    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n                              is a pandas Series of image data.\n    '''\n    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n    # for multiple data batches.  This amortizes the overhead of loading big models.\n    model = model_fn()\n    for content_series in content_series_iter:\n        yield featurize_series(model, content_series)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Application de l'udf d'extraction des features \u00e0 notre dataframe distribu\u00e9\nfeatures_df = images.repartition(24).select(col(\"path\"), col(\"label\"), featurize_udf(\"content\").alias(\"features\"))\nfeatures_df.show(5)", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+--------------+--------------------+\n|                path|         label|            features|\n+--------------------+--------------+--------------------+\n|s3://project-8-da...|Apple_Golden_1|[0.0, 0.027829845...|\n|s3://project-8-da...|Apple_Golden_1|[0.0, 0.12024237,...|\n|s3://project-8-da...|Apple_Golden_1|[0.0, 0.14426453,...|\n|s3://project-8-da...|Apple_Golden_1|[0.35117388, 0.06...|\n|s3://project-8-da...|Apple_Golden_1|[0.0036005825, 0....|\n+--------------------+--------------+--------------------+\nonly showing top 5 rows", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "La m\u00e9thode impl\u00e9mentant l'analyse en composantes principales appartient \u00e0 la classe RowMatrix. nous devant donc convertir la colonne features pour pouvoir lui appliquer cette m\u00e9thode"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# COnversion de la colonne features en RowMatrix\nrdd = features_df.select(col(\"features\")).rdd\n\nrdd_vectorized = rdd.map(lambda x: Vectors.dense(x))\n\nmatrix = RowMatrix(rdd_vectorized)", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "On peut maintenant lui appliquer l'ACP"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Application de l'ACP \u00e0 nos features\nn_comp = 4\n\npc = matrix.computePrincipalComponents(n_comp)\n\nprojected = matrix.multiply(pc)\n\ncollected = projected.rows.collect()", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Cr\u00e9ation du sch\u00e9ma pour le dataframe\nstruct_fields = [StructField(str(i), FloatType(), True) for i in range(1, n_comp+1)]\nschema = StructType(struct_fields)\n\n#Formatage des donn\u00e9e pour la cr\u00e9ation du dataframe\ndata = []\nfor l in collected:\n    data.append((float(l[0]),float(l[1]),float(l[2]),float(l[3])))\n\n#Cr\u00e9ation du dataframe\ndf = spark.createDataFrame(data, schema)\ndf.show(5)", "execution_count": 82, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "+---------+----------+---------+----------+\n|        1|         2|        3|         4|\n+---------+----------+---------+----------+\n|6.1351495| -6.020519|4.0145116|-1.9692829|\n| 4.928417| 4.6934485|2.3818104|-4.5944924|\n|6.9458036| -4.923555|6.6738276| -4.299764|\n| 6.596392|-5.4613843|5.9482656| -5.157067|\n| 6.412099|-2.3701758|2.6479127| -4.785862|\n+---------+----------+---------+----------+\nonly showing top 5 rows", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Il ne reste pluq qu'\u00e0 sauvegarder les r\u00e9sultats"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.write.csv(PATH_Result, header = False, mode = \"overwrite\")", "execution_count": 81, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "pygments_lexer": "python3"}}, "nbformat": 4, "nbformat_minor": 5}